#!/bin/bash
source ./config
mkdir -pv data


echo "Downloading NLCD 2011 Tree Canopy (cartographic)"
mkdir -pv data/canopy
CANOPYFILE="data/canopy/`basename $CANOPYURL`"
if [[ ! -f $CANOPYFILE ]] ; then
  wget -P data/canopy $CANOPYURL
else
  echo $CANOPYFILE exists. Skipping download.
fi


echo "Downloading NED data"
mkdir -pv data/ned13
for x in `seq $[-XMAX+1] $[-XMIN]` ; do
  for y in `seq $[YMIN+1] $[YMAX]` ; do
    URL=`printf $NED13URL $y $x`
    FILENAME="data/ned13/`basename $URL`"
    if [[ ! -f $FILENAME ]] ; then
      wget -P data/ned13 $URL
    else
      echo $FILENAME exists. Skipping download.
    fi
  done
done


echo "Downloading OSM Water Polygons"
mkdir -pv data/osm
if [[ ! -f data/osm/water-polygons-split-3857.zip ]] ; then
  wget -P data/osm http://data.openstreetmapdata.com/water-polygons-split-3857.zip
else
  echo "water-polygons-split-3857.zip exists. Skipping download"
fi


echo "Download NHD High Resolution"
# NOTE: This downloads ALL subregions, since there is no simple way to
# calculate which ones are needed. They are cropped to the relevant
# region before importing, however.
mkdir -pv data/nhd/hi
NHDURLS=`wget -O - ftp://nhdftp.usgs.gov/DataSets/Staged/SubRegions/FileGDB/HighResolution/ | egrep -o -E "ftp://nhdftp.usgs.gov:21/DataSets/Staged/SubRegions/FileGDB/HighResolution/NHDH...._931v220\.zip"`
for url in $NHDURLS ; do
  BASE=`basename $url .zip`
  DEST=data/nhd/hi/$BASE.gdb.zip
  echo $BASE
  if [[ ! -f $DEST ]] ; then
    wget -O $DEST "$url"
  else
    echo $DEST exists. Skipping download.
  fi
done
